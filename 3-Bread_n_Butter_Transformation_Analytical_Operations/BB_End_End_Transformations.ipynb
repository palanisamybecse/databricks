{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34461d3a-9f97-40db-ab1c-fef0a11d1fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c59e0da5-589c-43c9-92aa-88f25fc428f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee98f0c3-d120-4e76-bf48-369356dada9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Passive Data Munging** - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "    \n",
    "**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd552950-e07d-4179-b773-1473be40ed86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage1](stage1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b4147f7-6cc4-48bf-a3d9-0fb6232d6478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging - \n",
    "- Visible - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8aba2b3-1329-4fea-973a-3b01d3ce1df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality\n",
    "- - Null columns & null rows are there\n",
    "- - duplicate rows & Duplicate keys\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "497a7981-97cc-44af-9b15-29e9c3d5c100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66373673-efdb-4199-97d2-e7207b03f46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We have to define Spark Session to enter into Spark application\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"we47_Bread_n_Butter2_Important_Application\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000a2981-9fdb-437e-a7df-8d829db2f682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Exploration programatically\n",
    "rawdf1=spark.read.csv(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9108dbf-9d3b-4247-b248-84b6a324204a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Munging - EDA of schema/structure functions we can use\n",
    "rawdf1.printSchema()#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c11152-8bc1-4404-b847-7f948eeebeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Datamunging - EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - Standard deviation tells you how much the data varies from the average (mean).\n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given id column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fe08d49-f00d-46b4-919c-2fa4174b6002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f0e6e9b-b127-4448-8683-c59b59aaa628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Questions related to multiple files/paths/sub path handling\n",
    "#I have data in different filenames in a single/multiple location, i need to read all these data in a df - path=[\"path1/file1\",\"path1/file2\",\"path2/file3\"]\n",
    "#I have data in single pattern of file names in a single/multiple locations or subfolders, i need to read all these data in a df - path=[\"path1/\",\"path1/\",\"path2/\"], pathGlobFilter=\\\"custsm*\\\", recursiveFileLookup=True\n",
    "\n",
    "#Questions related handling evolving data structure with data ingested in different days/periods - Ans. Schema Evolution\n",
    "\n",
    "#Evolution is growth over the time (Filesystem level).. Eg. Source is sending data with additional columns week over week in csv format\n",
    "#1. Read and write in Serialized format( ORC,Parquet)\n",
    "#2. Read DF with mergeSchema = True\n",
    "\n",
    "#Questions related handling data from different sources with different related structure in a same day - Ans. Schema Merging/Melting (Dataframe level).. Eg. Source1 is sending custsmodified_NY with 5 columns and Source2 is sending custsmodified_TX with 4 columns\n",
    "#1. Read file1 in DF1, read file2 in DF2\n",
    "#2. Create DF3 by merging DF1 and DF2 using df1.unionByName(df2,allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac6ecf2a-2fb5-4581-bc98-ccd89ead2bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Combining Data + Schema Evolution/Merging (Structuring) - Preliminary Datamunging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f740b04-b13e-4f6b-ace2-67e8cb8335ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\\n\",\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custsmodified\")\n",
    "#2. Multiple files (with different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custsmodified\",\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custsmodifiedsample.txt\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/\",\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
