{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "681ec5fd-c942-44d7-b4e2-952512f3bf77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19e127d4-e32b-4092-8db1-528427662d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#By Knowing this notebook, we can become a eligible \"DATA EGRESS DEVELOPER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaaf1011-919a-4097-8988-59353259cf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Let's get some data we have already..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb66e97-8889-4d4c-bcbf-b0ef7a89ac52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\",header=True,inferSchema=True)\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0265fdfe-b452-4bd7-8c57-4b5b8da028c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing the data in Builtin - different file formats & different targets (all targets in this world we can write the data also...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd184b70-9145-46c8-ac5e-042e92e377ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Writing in csv format with few basic options listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5824e599-0a19-4cdd-8861-bd4fe6b531ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We did a schema migration from comma to tilde delimiter\\n\",\n",
    "df1.write.csv(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/csv_targetdata\",header=True,sep='~',mode='overwrite')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bec8562-255a-46b8-9a71-aac5d148d702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Writing in json format with few basic options listed below (Schema migration/Data Conversion)\n",
    "    path\n",
    "    mode\n",
    "    - We did a schema migration and data conversion from csv to json format (i.e structued to semi structured format)\\n\",\n",
    "    - json - we learn a lot subsequently, \n",
    "    - what is json - fundamentally it is a dictionary of dictionaries\n",
    "    - json - java script object notation\n",
    "    - format - {\"k1\":v1,\"k2\":v2,\"k3\":v2} where key has to be unique & enclosed in double quotes and value can be anything\n",
    "\n",
    "   **when to go with json or benifits** <br>\n",
    "    - a. If we have data in a semistructure format(variable data format with dynamic schema) <br>\n",
    "    - b. columns and the types and the order can be different <br>\n",
    "    - c. json will be provided by the sources if the data is dynamic in nature or if the data is api response in nature. <br>\n",
    "    - d. json is a efficient data format (serialized/encoded) for performing data exchange between applications via network & good for parsing also.<br>\n",
    "    - e. json can be used to group or create hierarchy of data in a complex or in a nested format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5d6b01-4a73-46cb-901b-c9bb71a99ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.json(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/json_targetdata\",mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d699c15-f4b4-4151-9563-341d00dfd5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3.Serialization & Deserialization File formats (Binary File formats)\n",
    "  What are the (builtin) serialized file formats we are going to learn? <br>\n",
    "    orc <br>\n",
    "    parquet <br>\n",
    "    delta(databricks properatory) <br>\n",
    "    <br>\n",
    "    - We did a schema migration and data conversion from csv/json to serialized data format (ie structued to sturctured(internall binary unstructured) format)<br>\n",
    "    - We learn/use a lot/heavily subsequently, <br>\n",
    "    - what is serialized - fundamentally they are intelligent/encoded/serialized/binary data formats applied with lot of optimization & space reduction strategies..<br>\n",
    "    - orc - optimized row column format <br>\n",
    "    - parquet - tiled data format <br>\n",
    "    - delta(databricks properatory) enriched parquet format - Delta (modified) operations can be performed <br>\n",
    "    - format - serialized/encoded , we can't see with mere eyes, only some library is used deserialized/decoded data can be accessed as structured data <br>\n",
    "    - **when to go with serialized or benifits** \n",
    "    - a. For storage benifits for eg. orc will save 65+% of space for eg. if i store 1gb data it occupy 350 mb space, with compression it can improved more... <br>\n",
    "    - b. For processing optimization. Orc/parquet/delta will provide the required data alone if you query using Pushdown optimization <br>\n",
    "    - c. Interoperability feature - this data format can be understandable in multiple environments for eg. bigquery can parse this data. <br>\n",
    "    - d. Secured\n",
    "    - **In the projects/environments when to use what fileformats - we learn in detail later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5486ebdf-65be-49d1-a1b5-dfad6c057095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.orc(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/orc_targetdata\",mode='ignore') #serialization\n",
    "spark.read.orc(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/orc_targetdata\").show(2) #deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1638400-2ad4-416b-b764-e22d868e8493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.option(\"maxRecordsPerFile\",1).parquet(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/parquet_targetdata2\",mode='error',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55404d49-e65e-4447-b629-968f68113810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df1.write.delta(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/delta_targetdata\")\n",
    "df1.write.format(\"delta\").save(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/delta_targetdata\",mode='overwrite')\n",
    "spark.read.format(\"delta\").load(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/delta_targetdata\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b97afc-1c53-4b35-b30b-e67440642e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#What is the default format of file will be generated with, when we don't mention the format explicitly?\n",
    "#It is Parquet(Delta)\n",
    "df1.write.save(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/what_targetdata\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1821e40-b194-444d-b073-2e2cd7848594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####4.Table Load Operations - Building LAKEHOUSE ON TOP OF DATALAKE\n",
    "  Can we do SQL operations directly on the tables like a database or datawarehouse? or Can we build a Lakehouse in Databricks? <br>\n",
    "  - We learn/use a lot/heavily subsequently, \n",
    "  - what is Lakehouse - A SQL/Datawarehouse/Query layer on top of the Datalake is called Lakehouse\n",
    "  - We have different lakehouses which we are going to learn further - \n",
    "    1. delta tables (lakehouse) in databricks\n",
    "    2. hive in onprem\n",
    "    3. bigquery in GCP\n",
    "    4. synapse in azure\n",
    "    5. athena in aws\n",
    "    - **when to go with lakehouse** - \n",
    "    - a. Transformation\n",
    "    - b. Analysis/Analytics\n",
    "    - c. AI/BI\n",
    "    - d. Literally we are going to learn SQL & Advanced SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50993128-04ff-459a-8609-6f1c0fb94102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Out of 18 write.functions, we know 9 functions, lets go with few more basic functions (xml, saveAsTable,InsertInto)\n",
    "df1.write.saveAsTable(\"default.customertbl\",mode='overwrite') #default delta format\n",
    "spark.read.table(\"default.customertbl\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabc7484-3d7b-421f-80c3-ef00bcec315c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Notes Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores the column names and just uses position-based resolution.\n",
    "# table has to be present already\n",
    "# this will be used for some minimal data write operation hence preferred function is saveAsTable()\n",
    "df1.write.insertInto(\"customertbl\",overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6f6e88-9fa7-4685-a60f-a8ac789c7b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####5. XML Format - Semi structured data format (most of the json features can be applied in xml also, but in DE world not so famous like json)\n",
    "- Used rarely on demand (by certain target/source systems eg. mainframes)\n",
    "- Can be related with json, but not so much efficient like json\n",
    "- Databricks provides xml as a inbuild function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6aeabf-2e2c-4837-9c25-5d03fad725bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.xml(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/xml_targetdata\",rowTag='customer',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff2f0b2-777b-4ed8-a575-3eff73fdebd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.xml(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/serialized_compressed_data_sources/xml_targetdata\",rowTag='customer').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98749571-55b9-432a-b111-0a609e915eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modes in Writing\n",
    "  1. **Append** - Adds the new data to the existing data. It does not overwrite anything.\n",
    "  2. **Overwrite** - Replaces the existing data entirely at the destination.\n",
    "  3. **ErrorIfexist**(default) - Throws an error if data already exists at the destination.\n",
    "  4. **Ignore** - Skips the write operation if data already exists at the destination."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3-Basic-WriteOps",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
