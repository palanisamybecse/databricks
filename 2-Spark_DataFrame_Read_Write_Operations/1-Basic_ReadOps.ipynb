{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f71998cc-5c07-454e-8f99-e981c9b21581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#By Knowing this notebook, we can become a eligible \\\"DATA INGESTION DEVELOPER\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c25be42b-d6e1-4f3d-9dfa-6f53f28fe656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### First Let's understand the basic Catalog + Volume Feature of Databricks\n",
    "Filesystem Hierarchy of Volume in Databricks (DBFS)?<br>\n",
    "Catalog Menu -> Catalog(Workspace)/schema(database)/volume(Datalake (DBFS))/folder/mere data files<br>\n",
    "Tables Hierarchy of Databricks?<br>\n",
    "Catalog Menu-> Catalog(Workspace)/schema(database)/tables(Lakehouse (Delta formatted DBFS))/structure+data(dbfs filesystem/some other filesystems)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2694923b-de38-49ae-8582-a838ec914e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Create - Catalog(Workspace)/schema(database)/volume(Datalake (DBFS))/folder/mere data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dbf8eb2-3947-4f15-bbb9-ab8bca617dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####If we need to create schema/volume/folder programatically, follow the below steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd58e38b-5568-4e47-b68d-69c0e19aeea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS pmadhan_catalog;\n",
    "CREATE SCHEMA IF NOT EXISTS pmadhan_catalog.pmadhan_schema;\n",
    "CREATE VOLUME IF NOT EXISTS pmadhan_catalog.pmadhan_schema.pmadhan_volume;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "112730ca-86bb-4dd7-823b-d0c73beb7691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Create Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7d1a76-9c08-4bb3-9c12-5623573a5a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e517fd5d-81be-4ffe-afb8-f42d48150d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Create some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f26249-627a-472b-ade6-d317ed8a0016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampledata=\"sample data to display\"\n",
    "dbutils.fs.put(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/samplefile.txt\",sampledata,True)\n",
    "dbutils.fs.ls(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1\")\n",
    "print(dbutils.fs.head(\"dbfs:///Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/samplefile.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f35f763-2e45-40b0-81be-518f796aedc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Spark SQL\n",
    "###1.E(Extract)\n",
    "L(Load)<br>\n",
    "Inbuilt libraries sources/targets & Inbuilt data Formats<br>\n",
    "2. Bread & Butter (T(Transformation) A(Analytical))<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f7637c-4b9d-494e-bb62-9e1e9180a9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Learn How to Create Dataframes from filesystem using different options\n",
    "Download the data from the below drive url <br>\n",
    "https://drive.google.com/drive/folders/1Tw7V9eBtUxy0xQMW38z3-bzWI_ewzLm6?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f91d2f9-879a-40e4-a634-4962bed0042c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###How Do We Write a Typical Spark Application (Core(Obsolute),SQL(Important),Streaming(Mid level important)) <br>\n",
    "####Before we Create Dataframe/RDD, what is the prerequisite? We need to create spark session object by instantiating sparksession class (by default databricks did that if you create a notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089379d9-a870-4bbd-ac99-184b6d19f359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d7073cc-6a4e-4d54-a72b-42af9a77e451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a DBFS volume namely commondata and upload the above data in that volume <br>\n",
    "What are other FS uri's (Uniform Resource Identified) available? file://, hdfs://, dbfs://, gs://, s3://, adls://, blob:// <br>\n",
    "The complete/absolute path with uri prefixed is called url (Uniform Resource Locator)...<br>\n",
    "This is a FS URL - dbfs:///Volumes/we47catalog/we47schema/we47_volume/we47_dir1/samplefile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07dc9033-e9e2-4de4-8993-68a5d0b27400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### How to Read/Extract the data from the filesytem and load it into the distributed memory for further processing/load - using diffent methodologies/options from different sources(fs & db) and different builtin function formats (csv/json/orc/parquet/delta/tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97c2f9d-3b02-45d3-a2ed-f7257b4d75d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"dbfs:///Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/samplefile.txt\")\n",
    "df1.show(20,False)#this show is a DSL function (works only in apache spark), pass number of rows to display and truncate of data upto 20 chars true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c63eab8-05d9-439d-ba24-e53f6a7efc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"dbfs:///Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs\")\n",
    "#df1.show(20,False)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345d2f7c-54c9-4184-a052-b6e22cf52396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####CSV Function complete options\n",
    "def csv(path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None, encoding: Optional[str]=None, quote: Optional[str]=None, escape: Optional[str]=None, comment: Optional[str]=None, header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None, mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None, multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> 'DataFrame'\"\n",
    "\n",
    "####If I don't use any options in this csv function, what is the default functionality?\n",
    " #1. By default it will consider ',' as a delimiter (sep='~') <br>\n",
    "#2. By default it will use _c0,_c1..._cn it will apply as column headers (header=True or toDF(\\\"\\\",\\\"\\\",\\\"\\\") or we have more options to see further) <br>\n",
    "#3. By default it will treat all columns as string (inferSchema=True or we have more options to see further) <br>\n",
    "#display with produce output in a dataframe format <br>\n",
    "#display with produce output in a beautified table format, specific to databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71f4a825-fd02-4950-b90a-3758dfc2cd74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sample data of custs_1:** <br>\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher <br>\n",
    "**Sample data of custs_header:**<br>\n",
    "custid,fname,lname,age,profession<br>\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4c9555-37cc-44a4-af36-536bf554b831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Header Concepts (Either we have define the column names or we have to use the column names from the data)\n",
    "#Important option is...\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, but we are asking spark to take the first row as header and not as a data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d678f8-6b53-4985-b3e9-bbbac99d3696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We use toDF if we want to add column name explicitly, if the data doesnt has column names or if we want to change the column name the data is provided with\\n\",\n",
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "df1.printSchema()\n",
    "df1.show(2)\n",
    "display(df1.limit(10))#Display is a special function in databricks, help us display in a table format with a limit that we can define to control the rows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9533ccb-8c12-488f-b7d2-a24dac433b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We use header option to consider the column name from the data itself\\n\",\n",
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\",header=True)\n",
    "#We use header option to consider the column name from the data itself and we can override the source given columns to our custom column names also by using header+ toDF()...\n",
    "print(df1.count())\n",
    "df1.printSchema()\n",
    "df1.show(2)\n",
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\",header=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"occupation\")\n",
    "print(df1.count())\n",
    "df1.printSchema()\n",
    "print(df1.count())\n",
    "df1.printSchema()\n",
    "df1.show(2)\n",
    "#display(df1.limit(10))#Display is a special function in databricks, help us display in a table format with a limit that we can define to control the rows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5373855-8945-4dc1-b78c-998d8c672d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Printing Schema (equivalent to describe table)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0200258a-d3b0-429c-a645-f808f86ca549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Inferring Schema \n",
    "# (Performance Consideration: Use this function causiously because it scans the entire data by immediately evaluating and executing\n",
    "# hence, not good for large data or not good to use on the predefined schema dataset)\n",
    "#sample data\n",
    "#4004979,Tara,Drake,32,\n",
    "#4004980,Earl,Hahn,34,Human resources assistant\n",
    "#4004981,Don,Jones,THIRTY SIX,Lawyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb4ad3d-fff9-455b-a465-c910e933e7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Performance Importance: Though inferSchema has to be used causiously, we can improve performance by using an option to reduce the data scanned for large data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558f850a-b2e9-403e-a321-70ed603256b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\",header=True,inferSchema=True)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ac87ba-d712-48a4-af1e-5771f9065d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#4. Using delimiter or seperator option\n",
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header_oth_del\",header=True,inferSchema=True,sep='~')\n",
    "\n",
    "#Do we need to put all these efforts?? No\n",
    "#df1.createOrReplaceTempView(\"view1\")\n",
    "#spark.sql(\"select split(`custid~fname~lname~age~profession`,'~') lst_of_cols from view1\").createOrReplaceTempView(\"view2\")\n",
    "#spark.sql(\"select lst_of_cols[0] id,lst_of_cols[1] fname,lst_of_cols[2] lname,lst_of_cols[3] age,lst_of_cols[4] profession from view2\").show(2)\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e074db70-33cb-4f36-adac-4f12379a10a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies (spark.read.inbuiltfunction or spark.read.format(anyfunction).load(\"path\")) with 3 ways of creating dataframes (pass parameters to the csv()/option/options))\n",
    "#Methodology #1 (spark.read.inbuiltfunction) we use to create dataframe from builtin sources\n",
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header_oth_del\",header=True,inferSchema=True,sep='~')\n",
    "print(df1.count())\n",
    "#Methodology #2 (spark.read.format('anysources/anyformats').load()) we use to create dataframe from builtin sources\n",
    "df1_bq=spark.read.format(\"csv\").load(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\")\n",
    "print(df1_bq.count())\n",
    "\n",
    "#Mostly we use option or options for external sources, not for builtin sources...\n",
    "#option can be used for 1 or 2 option...\n",
    "df1=spark.read.option(\"header\",\"true\").option(\"sep\",\"~\").csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header_oth_del\")\n",
    "#or\n",
    "df1_bq=spark.read.option(\"header\",\"true\").format(\"csv\").load(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\")\n",
    "print(df1_bq.count())\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "df1=spark.read.options(header=True,sep=\"~\",inferSchema=True).csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header_oth_del\")\n",
    "print(df1.count())\n",
    "#or\n",
    "df1_bq=spark.read.options(header=True,sep=\"~\",inferSchema=True).format(\"csv\").load(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\")\n",
    "print(df1_bq.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb932846-1a4d-46c0-a260-11b53bb0f47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generic way of read and load data into dataframe using fundamental options from BUILT in sources (csv/orc/parquet/xml/json/table) (inferschema, header, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86c6391-7f15-4794-9088-66dcfc7c8f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Methodology #1 (spark.read.inbuiltfunction) we use to create dataframe from builtin sources\n",
    "df1=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header_oth_del\",header=True,inferSchema=True,sep='~')\n",
    "print(df1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47150565-a73f-4c3b-86f0-2425eb6bf6cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generic way of read and load data into dataframe using extended options from EXTERNAL sources (bigquery/redshift/athena/synapse) (tmpfolder, access controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032ca061-b144-4232-951e-566667dcb3f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#options can be used for multiple options in one function as a parameter...\n",
    "#Methodology #2 (spark.read.format('anysources/anyformats').load()) we use to create dataframe from builtin sources\n",
    "df1_bq=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\")\n",
    "df1_bq=spark.read.options(header=True,sep=\"~\",inferSchema=True).format(\"csv\").load(\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs_header\")\n",
    "print(df1_bq.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1932de3-bb3d-4706-9fa9-4d281d3d61e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading data from multiple files & Multiple Path (We still have few more options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a5374c-e8db-4c51-a959-355582ba9b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df1_multiple_files=spark.read.csv(path=\"/Volumes/workspace/default/volume_pmadhan_datalake/custs*\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(df1_multiple_files.count())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815e6f99-d09a-488f-b887-1237dee7030f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#What if we have files in multiple names in multiple paths... Below code can read data from multiple paths and multiple files...\n",
    "#Caveate: while reading from multiple csv files.. data patterns like type of column and number of columns should always be same\n",
    "df1_multiple_files=spark.read.csv(path=[\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1//custs*\",\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1//prospect*\"],inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(df1_multiple_files.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea825c1-7d40-4243-87a2-3a4aacd1ad03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What if while reading from multiple csv files.. data patterns like type of column and number of columns are not same? We learn this using some feature called schema evolution using few functions like mergeSchema & unionByName in Transformation part.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b22edc-b341-4a42-81db-ea9d334b180c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Requirement: I am getting data from different source systems of different regions (NY, TX, CA) into different landing pad (locations), how to access this data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b79971c-c474-47f5-95b0-5ae9f50b3f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#recursiveFileLookup is a parameter that can be used to read data from multiple subdirectories\n",
    "#pathGlobFilter is the parameter used for matching a given pattern under the main/subdirectories\n",
    "df1_different_subdirs=spark.read.csv(\"/Volumes/workspace/default/volume_pmadhan_datalake/\",recursiveFileLookup=True,pathGlobFilter=\"custs*\",header=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "df1_different_subdirs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44501519-7e09-49c1-9762-452c75e96323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Provide schema with SQL String or programatically (very very important)\n",
    "We learned so far to define column names explicitly(toDF) or define column names implicitly(header) or infer schema <br>\n",
    "If we want to handle both column names and structure(schema) explicitly in one shot, we have 2 methods to achieve that... <br>\n",
    "df1_different_subdirs=spark.read.csv(\\\"/Volumes/workspace/default/volumewe47_datalake/wd36_source/\\\",header=True,inferSchema=True)<br>\n",
    "[PySpark SQL Datatypes](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) <br>\n",
    "[Data Types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html)<br>\n",
    "\n",
    "**###To provide schema (columname & datatype), what are the 2 basic options available that we learned so far ? inferSchema/toDF<br>\n",
    "  ###We are going to learn additionally 2 more options to handle schema (colname & datatype)?<br>\n",
    "  ###1. Using simple string format of define schema.<br>\n",
    "  ###IMPORTANT: 2. Using structure type to define schema.<br>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6cfb02-a901-4b99-b325-5b1091f0ada3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "###Option 1. Using simple string format of define schema. (least used because it will not support some complex formats)\n",
    "datastruct=\"id int,fname string,lname string, age int,prof string\"\n",
    "df0=spark.read.csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs1\",sep=\"~\",inferSchema=True,header=False)\n",
    "df0.display()\n",
    "df1=spark.read.schema(datastruct).csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs1\",sep=\"~\",Header=True)#we didnt use inferSchema or .toDF\n",
    "df1.printSchema()\n",
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1e98a7-a37e-4a94-bcc3-ae55af799722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTANT: 2. Using structure type to define schema.\\n\",\n",
    "#For the given data \\\"id,fname,lname,age,prof\\\" i am going to use StructType(StructField(\\\"id\\\",IntegerType()),StructField(\\\"fname\\\",StringType())...)\\n\",\n",
    "#Pattern: StructType([StructField(\\\"colname\\\",DataType()),StructField(\\\"colname\\\",DataType())......])\\n\",\n",
    "#StructType - 1 time\\n\",\n",
    "#StructField - total number of fields time\\n\",\n",
    "#IntegerType - 2 times\\n\",\n",
    "#StringType - 3 times\\n\",\n",
    "#4000001|Kristina|Chung|55|Pilot|\\n\",\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType()),StructField(\"lname\",StringType()),StructField(\"age\",IntegerType()),StructField(\"profession\",StringType())])\n",
    "df1=spark.read.schema(custom_schema).csv(path=\"/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/custs1\",sep='~',header=True)#we didnt use inferSchema or .toDF\\n\",\n",
    "df1.printSchema()\n",
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf188545-d46a-4817-b129-1ee992bfe32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading data from different builtin formats (csv, json, xml, orc, parquet, delta, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e6dfb2-bd3d-4941-90e0-3c840db57031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98bb641-1974-49ce-80b6-05306722a199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unable to find files in this location\n",
    "df1=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/csv_targetdata\",header=True,sep='~')\n",
    " #/Volumes/pmadhan_catalog/pmadhan_schema/pmadhan_volume/directory1/\n",
    "display(df1.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9290bb25-2522-4917-83a6-1590eaad91f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Reading data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118d9867-0857-484d-9920-1d7da55cd423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json=spark.read.json(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/json_targetdata\")\n",
    "display(df_json.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ef5f576-0d71-412a-b514-30d5b61da715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Reading data from xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f66737f-b4f8-47bc-b181-c4ce614ce1ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_xml=spark.read.xml(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/xml_targetdata\",rowTag=\"customer\")\n",
    "display(df_xml.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0851a5-6b0d-4454-8eec-1b3c294bff1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Reading data from Serialized format - orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a086a515-28dd-45ed-97a9-7290512435e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orc=spark.read.orc(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/orc_targetdata\")\n",
    "display(df_orc.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "344765e6-0a26-4898-95c9-d198362cc4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Reading data from Serialized format - parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e7bd6e7-c728-41dd-bb00-e66ebf7063d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet=spark.read.parquet(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/parquet_targetdata\") #DSL FBP methodology\n",
    "display(df_parquet.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70eb2da6-6255-407e-8cc2-b9051d592862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Reading data from Serialized format - delta(parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "439153bc-b49a-40ce-8731-9909ef08b079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_delta_parquet=spark.read.format(\"delta\").load(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/delta_targetdata\")\n",
    "display(df_delta_parquet.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0143631f-6fd1-420c-8ad7-9335182377f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Reading data from table - delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "464abcd7-68d4-4733-9d1e-dcaeddb7d657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_delta_table=spark.read.table(\"default.cust\")\n",
    "df2=df_delta_table.select(\"customerid\",\"BranchID\",\"Address\",\"DateOfBirth\").filter(\"branchid=115\")#Domain Specific Lang Function based programming\n",
    "display(df2.take(10))\n",
    "#or directly write query\n",
    "df1=spark.sql(\"select customerid,branchid,address,dateofbirth from default.cust where branchid=115\") #SQL Declarative Query\n",
    "display(df1.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc95a86f-1dc1-4be1-ba75-dd6f0252a285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " What are all the overall functions & options we used/learned in this notebook, for learning fundamental spark csv read operations? <br>\n",
    "    - **The concept of catalog & volume...** <br>\n",
    "    - spark.read.csv(path=\\\"\\\") <br>\n",
    "    - spark.read.csv(path=[]) <br>\n",
    "    - **Basic (important) options** - inferSchema=True, sep='~', header=True, toDF(), schema <br>\n",
    "    - **Additional options** - recursiveFileLookup, pathGlobalFilter.<br>\n",
    "    - **Other options based on questions** - linesep, sampling ratio, unionByName,allowMissingColumns.<br>\n",
    "    - spark.read.format(\\\"\\\").load()<br>\n",
    "    - spark.read.option(k,v).format(\\\"\\\").load()<br>\n",
    "    - spark.read.options(k=v,k=v).format(\\\"\\\").load()<br>\n",
    "    - Custom schema:<br>\n",
    "    - strstruct=\\\"colname datatype,colname datatype <br>\n",
    "    - strcustomstruct=StructType([StructField(\\\"colname\\\",DataType()),....]) <br>\n",
    "    - How to read data from different builtin formats (csv, json, xml, orc, parquet, delta, table) <br>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8319610781991548,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Basic_ReadOps",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
