{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76341f47-13b6-496f-baba-ccda21262eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9c5442-7c69-4a29-a205-f76a50a30b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Welcome to Inceptez Technologies,\n",
    "Let us understand about creating notebooks & magical commands\n",
    "https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png\n",
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b37c36-e43b-44dc-a960-062e06143391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Let us learn first about Magical Commands\n",
    "**Important Magic Commands**\n",
    "    - %md: allows you to write markdown text to design the notebook.<br>\n",
    "    - %run: runs a Python file or a notebook.<br>\n",
    "    - %sh: executes shell commands on the cluster nodes.<br>\n",
    "    - %fs: allows you to interact with the Databricks file system (Datalake command)<br>\n",
    "    - %sql: allows you to run Spark SQL/HQL queries.<br>\n",
    "    - %python: switches the notebook context to Python.<br>\n",
    "    - %pip: allows you to install Python packages.<br>\n",
    "    <br>\n",
    "**Not Important Magic Commands or We learn few of these where we have Cloud(Azure) dependency**\n",
    "- %scala: switches the notebook context to Scala.\n",
    "- %r: switches the notebook context to R.\n",
    "- %lsmagic: lists all the available magic commands.\n",
    "- %config: allows you to set configuration options for the notebook.\n",
    "- %load: loads the contents of a file into a cell.\n",
    "- %who: lists all the variables in the current scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d021d8ac-bca7-40ec-b807-8afeb1b809e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to call a notebook from the current notebook using %run magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a2d4ed-dbcd-47e5-893b-47cca7f2c357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/palanisamy744@gmail.com/databricks/Markdown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ba94a67-817c-4cd4-a4e9-a79beec617af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a linux commands inside a notebook using %sh magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec062620-3ab9-4be9-b78d-63f3460a4544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -l #/databricks-datasets/airlines\n",
    "  #  \"head -1 /databricks-datasets/airlines/part-01902\\n\",\n",
    "  #  \"echo \\\"head completed\\\"\\n\",\n",
    "  #  \"tail -1 /databricks-datasets/airlines/part-01902\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b564777-66e8-4a10-8e33-1ac88bbb947f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We are going to use Databricks Unity Catalog (We don't know about it yet)\n",
    "to create tables and files under the volume (catalog/schema/volume/folder/files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7bd9e7-c21f-445a-b0b7-e6bccb780161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.volume_pmadhan_datalake;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e2044c-6ebb-4533-a593-1c07c9f71545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Upload some sample data going into (Catalog -> My Organization -> Workspace -> Default -> Volumes) <br> How to run a DBFS (like Hadoop) FS commands inside a notebook using %fs magic command to copy the uploaded data into some other volume from the uploaded volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a33f51a-b48e-4f3e-8d7d-52e9a58c976f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls \"dbfs:///Volumes/workspace/default/volume_pmadhan_datalake/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f4a322-950e-4562-93a9-8fe24c761896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs cp \"dbfs:/Volumes/workspace/default/volume_pmadhan_datalake/patients.csv\" \"dbfs:/Volumes/workspace/default/volume_pmadhan_datalake/patients_copy.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aad8d53b-ed88-4685-b367-1f082cfd9009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Learning for the first time the dbutils, we learn in detail later <br>\n",
    "Rather using fs command, we can use databricks utility command (comprehensive) to copy the data/any other filesystem operations in the DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f412d02e-c23c-4af3-a678-1ffbd55284d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.cp(\"dbfs:/Volumes/workspace/default/volume_pmadhan_datalake/patients.csv\",\"dbfs:/Volumes/workspace/default/volume_pmadhan_datalake/patients_copy2.csv\")\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/workspace/default/volume_pmadhan_datalake/patients_copy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab99b33-927b-4e21-aea4-0dd484758ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Spark SQL/HQL Queries inside a notebook using %sql magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6ba0a2-c583-46d7-8cc8-b8a85497e7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists default.city2(id int,city string);\n",
    "insert into default.city2 values(3,'Mumbai'),(4,'Lucknow');\n",
    "select * from city2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f54a1c19-8885-4f4f-a4e0-75d36aab3d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#OOPS, FBP & Declarative (SQL)\n",
    "spark.sql(\"select * from city2\").explain(True)\n",
    "spark.sql(\"select * from city2\").display()\n",
    "spark.sql(\"select * from city2\").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8932f934-ca4f-4086-9c48-9084098ba487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "update city2 set city='Kolkata' where id=4;\n",
    "from city2 select *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b951f7d-f203-403b-9eeb-9bc56e764509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show create table city2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1204089e-f0e3-4a62-8787-4b9e28b7c66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Python Program inside a notebook using %python  magic command or by default the cell will be enabled with python interpretter only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05e3ee1-a829-4e2d-9a91-e97e0f682979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sqrt(a):\n",
    "    return a*a\n",
    "\n",
    "print(\"square root function call \",sqrt(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8947f490-fccd-44c3-adf3-89a228ed82aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### In the python magic cell itself, we already have spark session object instantiated, <br>\n",
    "so we can lavishly write spark programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00db6177-e64b-46be-a47e-9ce4d3e9f2bd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765648839218}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.session import SparkSession\n",
    "#import pyspark.sql.session as sprk\n",
    "print(spark)\n",
    "spark1 = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(spark1)\n",
    "df1 = spark1.read.csv(\"/Volumes/workspace/default/volume_pmadhan_datalake/patients.csv\",header=True)\n",
    "display(df1)\n",
    "#DSL - Domain specific language\\n\",\n",
    "df1.where(\"married='Yes'\").write.saveAsTable(\"default.we47_patients\")\n",
    "spark.sql(\"select count(1) as cnt,InPatient from default.we47_patients group by 2\").explain(True)\n",
    "spark.sql(\"select count(1) as cnt,InPatient from default.we47_patients group by 2\").display()\n",
    "# DBTITLE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b006d7-118c-47f9-9d87-aa9c8c4958a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"default.we47_patients\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00957679-2b32-44f4-bbd8-4df582276fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to install additional libraries in this current Python Interpreter using %pip magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7cdb541-702a-4eca-a964-35f9784e0b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pypi"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6418327900742671,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Magic Commands",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
